# -*- coding: utf-8 -*-
"""Lumbar Vertebrae Classification.ipynb

Automatically generated by Colab.

Original file is located at
https://colab.research.google.com/drive/1A1hFzPgOQDtQycazuKeWpDS01WgjATJh
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pandas as pd
import sklearn
import tensorflow as tf
from urllib.parse import urljoin

"""#mount data"""

#load data
from google.colab import drive
drive.mount('/content/drive')

base_path = "/content/drive/MyDrive/vertebrates_data/"
train_path = urljoin(base_path, "processed_with_GAN/")
test_path = urljoin(base_path, "processed/test/")

print(train_path)
print(test_path)

"""## visualize a sample of the dataset"""

class_names = ['L1 SUPERIOR', 'L2 SUPERIOR', 'L3 SUPERIOR','L4 SUPERIOR','L5 SUPERIOR']
plt.figure(figsize = (20,10))
for i,c in enumerate(class_names):
    path = train_path + c + '/'
    fnames = os.listdir(path)
    with open(path+fnames[0], 'rb') as f:
        img = mpimg.imread(f)
    plt.subplot(1,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(img)
    # The CIFAR labels happen to be arrays,
    # which is why you need the extra index
    plt.title(c)
plt.show()

"""Define the train, test and validation data generators"""

from keras.preprocessing.image import ImageDataGenerator


test_size = 72
batch_size = 16
epochs = 20
target_size = (240, 240) #resize all images to 240x240
learning_rate = 1e-4

#preprocessing function - in inception values expected are in [-1,1] and not in [0,1]
# this should replace the rescale option
def normalize(img):
    img1 = 2*np.array(img)/255.0 -1
    return img1

train_datagen = ImageDataGenerator(preprocessing_function=normalize,
                                   zoom_range=0.1,
                                   horizontal_flip=True,
                                   brightness_range = [0.8, 1.2], # [0.85, 1.4],
                                   validation_split=0.05)
test_datagen = ImageDataGenerator(preprocessing_function=normalize)

# The list of classes will be automatically inferred from the subdirectory names/structure under train_dir
train_generator = train_datagen.flow_from_directory(
                  train_path,
                  target_size=target_size,
                  batch_size=batch_size,
                  class_mode='categorical',
                  subset='training')

num_classes = train_generator.num_classes

validation_generator = train_datagen.flow_from_directory(
                       train_path,
                       target_size=target_size,
                       batch_size=batch_size,
                       class_mode='categorical',
                       subset='validation')

"""Load pretrained model - up to some intermediate layer"""

import keras
from keras.applications.inception_v3 import InceptionV3
from keras.models import Model, load_model
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers


conv_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(240, 240, 3))
last_layer_name = 'mixed8'  #| 'mixed7', 'mixed8', 'mixed9', 'all'
#mixed7  17 x 17 x 768   mixed8  8 x 8 x 1280  mixed9 8 x 8 x 2048

last_layer = conv_base.get_layer('mixed8')
last_output = last_layer.output
last_output = keras.layers.Flatten()(last_output)
print('last layer output shape: ', last_layer.output_shape)
model_tl = Model(conv_base.input, last_output)
model_tl.trainable = False
for layer in model_tl.layers:
    layer.trainable = False

model = Sequential()
model.add(model_tl)
# hidden layer
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
# output layer
model.add(Dense(train_generator.num_classes, activation='sigmoid'))

opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

"""Data Pre-processing and Data Augmentation"""

print("num clases", train_generator.num_classes)
print("samples", train_generator.samples)
x, y = train_generator.next()
for xi in x:
  plt.imshow(xi)
  plt.title("just a sample of a transformed image")
  plt.show()

"""Model Checkpoints"""

from keras.callbacks import *
filepath = base_path + "MyCNN/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5"
checkpoint = ModelCheckpoint(filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=True,
                             save_freq='epoch',
                             mode='max')
callbacks_list = [checkpoint]

history = model.fit(train_generator,
                    validation_data=validation_generator,
                    steps_per_epoch=train_generator.samples//batch_size,
                    epochs=epochs,
                    validation_steps=validation_generator.samples//batch_size,
                    verbose=1,
                    shuffle=True)

# Save the final trained model to a file
model_weight_file = base_path + "MyCNN/vnetwork_model.h5"
model.save(model_weight_file)

# Model evaluation
from keras.models import Model, load_model
import numpy as np

load_model_from_file = True
if load_model_from_file:
    model_weight_file = base_path + "MyCNN/vnetwork_model.h5"
    model = load_model(model_weight_file)
scores_train = model.evaluate(train_generator,verbose=1)
scores_validation = model.evaluate(validation_generator,verbose=1)
print("Train Accuracy: %.2f%%" % (scores_train[1]*100))
print("Validation Accuracy: %.2f%%" % (scores_validation[1]*100))

#For plotting Accuracy and Loss
def LearningCurve(history):
    # summarize history for accuracy
    plt.figure(figsize=(10,3.5))
    plt.subplot(1,2,1)
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
# summarize history for loss
    plt.subplot(1,2,2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

LearningCurve(history)

def evaluate_model(test_path, target_size, batch_size, test_size,
                   model_weight_file, model, load_model_from_file=False):

  # We take the ceiling because we do not drop the remainder of the batch
  compute_steps_per_epoch = lambda x: int(np.ceil(1. * x / batch_size))
  test_steps = compute_steps_per_epoch(test_size)
  test_generator = test_datagen.flow_from_directory(
                  test_path,
                  target_size=target_size,
                  batch_size=batch_size,
                  class_mode=None,
                  shuffle=False)
  test_generator.reset()

  # Calling the saved model for making predictions
  if load_model_from_file:
      tl_img_aug_cnn = load_model(model_weight_file)
  else:
      tl_img_aug_cnn = model
  pred = tl_img_aug_cnn.predict(test_generator,
                              verbose=1,
                              steps=test_steps)
  predicted_class_indices = np.argmax(pred, axis=1)
  labels = (test_generator.class_indices)
  labels = dict((v,k) for k,v in labels.items())
  predictions = [labels[k] for k in predicted_class_indices]
  filenames = test_generator.filenames
  # results = pd.DataFrame({"Filename":filenames, "Predictions":predictions})
  return filenames, predictions, pred

filenames, predictions, pred = evaluate_model(test_path, target_size,
                      batch_size, test_size, model_weight_file, model,
                                              load_model_from_file=True)

import seaborn as sns

def PerformanceReports(conf_matrix, class_report, labels):
    ax = plt.subplot()
    sns.heatmap(conf_matrix, annot=True,ax=ax)
    # labels, title and ticks
    ax.set_xlabel('Predicted labels')
    ax.set_ylabel('True labels')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels(labels)
    ax.yaxis.set_ticklabels(labels)
    plt.show()
    ax= plt.subplot()
    sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T,
                annot=True,ax=ax)
    ax.set_title('Classification Report')
    plt.show()

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score


def show_confusion_matrix(test_labels, predictions, pred, train_generator):
  labels = ['L1','L2','L3','L4', 'L5']
  cm = confusion_matrix(test_labels, predictions)
  print(cm)
  cr = classification_report(test_labels, predictions)
  class_report = classification_report(test_labels, predictions,
                                    target_names=labels,
                                    output_dict=True)
  print(cr)
  PerformanceReports(cm, class_report, labels)
  predicted_class_indices = np.argmax(pred, axis=1)

  labels = (train_generator.class_indices)
  labels = dict((v,k) for k,v in labels.items())
  predictions = [labels[k] for k in predicted_class_indices]

test_labels = [fn.split('/')[0] for fn in filenames]
show_confusion_matrix(test_labels, predictions, pred, train_generator)

"""print the names of the examples the model erred"""

for i in range(len(test_labels)):
    if test_labels[i] != predictions[i] :
        print(i, filenames[i], test_labels[i], predictions[i])

"""# Train the whole network for a few iteration"""

from keras import backend as K
model_tl.trainable = True
for layer in model_tl.layers:
   layer.trainable = True

#change only lerning rate
K.set_value(model.optimizer.learning_rate, 0.1e-4)

print(model.summary())

epochs = 10
history = model.fit(
         train_generator,
          steps_per_epoch=train_generator.samples//batch_size,
          validation_data=validation_generator,
          validation_steps=validation_generator.samples//batch_size,
          epochs=epochs,
          verbose=1,
          shuffle=True)
model_weight_file = base_path + "MyCNN/vnetwork_model_retrained_mix8d.h5"
model.save(model_weight_file)

# Model evaluation
from keras.models import Model, load_model
import numpy as np

scores_train = model.evaluate(train_generator, verbose=1)
scores_validation = model.evaluate(validation_generator, verbose=1)
print("Train Accuracy: %.2f%%" % (scores_train[1]*100))
print("Validation Accuracy: %.2f%%" % (scores_validation[1]*100))

# plot the Accuracy and Loss
LearningCurve(history)

# Show confusion matrix

filenames, predictions, pred = evaluate_model(test_path, target_size,
                      batch_size, test_size, model_weight_file, model,
                                              load_model_from_file=False)

test_labels = [fn.split('/')[0] for fn in filenames]
show_confusion_matrix(test_labels, predictions, pred, train_generator)

"""# Analysis - which parts of the image most affects the classification"""

import time
import cv2 as cv
from collections import OrderedDict


OCCLUDED_PATH = "/content/drive/MyDrive/vertebrates_data/analysis/occluded/"
OCC_TEST_PATH = urljoin(OCCLUDED_PATH, "test/")
OCC_MASKED_PATH = urljoin(OCCLUDED_PATH, "masked/")



def occlude_sizes(is_symmetric=False):
  accuracy_dict = OrderedDict()
  for mask_size in [(32, 32), (64, 64)]: # (16, 16)]
    accuracy_dict[mask_size] = occlude_locations(mask_size, is_symmetric)
  print(accuracy_dict)
  return accuracy_dict


def occlude_locations(mask_size, is_symmetric=False):
  # Get image dimensions
  image_height, image_width, channels = (240, 240, 3)

  # Ensure mask fits within image bounds
  mask_height, mask_width = mask_size
  stride = int(mask_height / 2)
  assert mask_height <= image_height and mask_width <= image_width

  results_dict = OrderedDict()
  # Loop through the image with stride steps
  iter = 0
  if is_symmetric:
    stop = int((image_width - mask_width)/2) + 1
  else:
    stop = image_width - mask_width + 1

  for y in range(0, image_height - mask_height + 1, stride):
    for x in range(0, stop, stride):
      if (x,y) in results_dict.keys():
        continue
      occlude_all_images(mask_height, mask_width, channels, x, y, is_symmetric)
      # wait 10 seconds
      time.sleep(10)
      iter += 1

      acc = get_accuracy()
      print('mask_size: {}, iteration {}: x = {}, y = {}, accuracy: {}'.format(mask_size, iter, x, y, acc))
      if is_symmetric:
        print('mask_size: {}, iteration {}: x = {}, y = {}, accuracy: {}'.format(mask_size, iter, image_width - x - mask_width, y, acc))
      results_dict[(x,y)] = acc

  cv.waitKey(0)
  cv.destroyAllWindows()
  # sort the results dictionary by accuracy, increasing order
  print('Sorted coordinates by accuracy, increasing order: {}'.format(sorted(results_dict, key=results_dict.get)))
  return results_dict


def occlude_all_images(mask_height, mask_width, channels, x, y, is_symmetric=False):
  class_names = ['L1 SUPERIOR/', 'L2 SUPERIOR/', 'L3 SUPERIOR/', 'L4 SUPERIOR/', 'L5 SUPERIOR/']
  for i, c in enumerate(class_names):
    path = urljoin(OCC_TEST_PATH, c)
    fnames = os.listdir(path)

    for filename in fnames:
      path = urljoin(OCC_TEST_PATH, urljoin(c, filename))
      # print('path: ' + path)
      output = urljoin(OCC_MASKED_PATH, urljoin(c, filename))
      # print('output: ' + output)

      # for each test image:
      image = cv.imread(path)
      occlude_single_image(image, mask_height, mask_width, channels, x, y, output, is_symmetric)


def occlude_single_image(image, mask_height, mask_width, channels, x, y, output_path, is_symmetric=False):
  # Create a copy of the image
  occluded_img = image.copy()

  # Create occlusion mask (all black pixels within defined size)
  mask = np.zeros((mask_height, mask_width, channels), dtype=image.dtype)

  # Apply mask to current region
  occluded_img[y:y+mask_height, x:x+mask_width] = mask

  if is_symmetric:
    x = 240 - x - mask_width
    occluded_img[y:y+mask_height, x:x+mask_width] = mask

  # save
  cv.imwrite(output_path, occluded_img)


def get_accuracy():
  labels = ['L1','L2','L3','L4', 'L5']
  filenames, predictions, pred = evaluate_model(OCC_MASKED_PATH, target_size,
                        batch_size, test_size, model_weight_file, model,
                                                load_model_from_file=False)

  test_labels = [fn.split('/')[0] for fn in filenames]
  class_report = classification_report(test_labels, predictions,
                                    target_names=labels, output_dict=True)
  return class_report.get('accuracy')

accuracy_dict = occlude_sizes(is_symmetric=True)

print(accuracy_dict)